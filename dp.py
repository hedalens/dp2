# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VgBQvy4tylE5Jw6qwK6HkMozlZEQ2wbx
"""

import bz2
import subprocess
import re
import xml.sax

pip install wikitextparser

import wikitextparser as wtp
from wikitextparser import remove_markup

pip install mwparserfromhell

import mwparserfromhell

from google.colab import drive
drive.mount('/content/drive')

class WikiXmlHandler(xml.sax.handler.ContentHandler):
    """Content handler for Wiki XML data using SAX"""
    def __init__(self):
        xml.sax.handler.ContentHandler.__init__(self)
        self._buffer = None
        self._values = {}
        self._current_tag = None
        self._pages = []

    def characters(self, content):
        """Characters between opening and closing tags"""
        if self._current_tag:
            self._buffer.append(content)

    def startElement(self, name, attrs):
        """Opening tag of element"""
        if name in ('title', 'text'):
            self._current_tag = name
            self._buffer = []

    def endElement(self, name):
        """Closing tag of element"""
        if name == self._current_tag:
            self._values[name] = ' '.join(self._buffer)

        if name == 'page':
            self._pages.append((self._values['title'], self._values['text']))
#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c

# Object for handling xml
handler = WikiXmlHandler()# Parsing object
parser = xml.sax.make_parser()
parser.setContentHandler(handler)# Iteratively process file
for line in subprocess.Popen(['bzcat'], 
                              stdin = open("/content/drive/My Drive/Colab Notebooks/etwiki-latest-pages-articles.xml.bz2"), 
                              stdout = subprocess.PIPE).stdout:
    parser.feed(line)
#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c

all_pages = dict(handler._pages)
all_titles = list(all_pages)
#
disambiguation_pages_texts = []
for el in handler._pages:
  is_disam_page = '{{täpsustus}}' in el[1] or '{{täpsustuslehekülg}}' in el[1] or '{{Täpsustus}}' in el[1] or '{{Täpsustuslehekülg}}' in el[1]
  if (is_disam_page) and el[0] != 'Vikipeedia:Vormistusreeglid':
    disambiguation_pages_texts.append(el)
#

#make list of (disambiguation page, senses)-s
#
disambiguation_pages_senses = []
for el in disambiguation_pages_texts:
  senses = []
  v = el[1].split('Vaata ka')[0].split('*')
  for s in v:
    links = re.findall("\[\[(.*?)\]\]",s)
    if links != []:
      senses.append(links[0].split('|')[0])
  disambiguation_pages_senses.append((el[0],senses))
#

pip install stanza

import stanza

stanza.download('et', verbose=False)

et_nlp = stanza.Pipeline('et', processors='tokenize,lemma', verbose=False)

def trim_unnessessary_spaces(text):
  splitted = text.split("\n")
  for i, item in enumerate(splitted):
    splitted[i] = item.lstrip()
    splitted[i] = re.sub('< ','<',splitted[i])
    splitted[i] = re.sub(' >','>',splitted[i])
    s_l = re.findall('& [a-z1-9]+;',splitted[i])
    for el in s_l:
      one = el.split()[0]
      two = el.split()[1]
      three = one + two
      splitted[i] = re.sub(el,three,splitted[i])
    splitted[i] = re.sub('& #','&#',splitted[i])     
  text = '\n'.join(splitted)
  return text
def search(text):
  regex = re.compile(r'==\s*Vaata ka\s*')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Kirjandus\s*==')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Viited\s*==')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Välislingid\s*==')
  text = regex.split(text)[0]
  code = mwparserfromhell.parse(trim_unnessessary_spaces(text))
 
  for tag in code.filter_tags(recursive=False):
    if tag[0] == "{" and tag[-1] == "}" or tag.tag == 'gallery' or tag.tag == 'imagemap' or tag.tag == 'center': #or tag.tag == 'ref'
      code.replace(tag,"")
    else:
      code.replace(tag,tag.contents)
  
  for argument in code.filter_arguments():
    code.replace(argument, "")
  
  for comment in code.filter_comments():
    code.replace(comment,"")
  
  for external_link in code.filter_external_links():
    code.replace(external_link,"")
  
  for heading in code.filter_headings():
    code.replace(heading,"")
  
  for html_entity in code.filter_html_entities():
    code.replace(html_entity, html_entity.normalize())
  
  for template in code.filter_templates(recursive=False):
    code.replace(template,"")
  
  for wikilink in code.filter_wikilinks(recursive=False):
    if bool(re.match("(File|Fail|Pilt|Image):.+\.(SVG|svg|JPEG|jpeg|GIF|gif|PNG|png|JPG|jpg)",str(wikilink.title))):
      code.replace(wikilink,"")

  answer = remove_markup(str(code))

  splitted = answer.split('\n')
  for i,item in enumerate(splitted):
    splitted[i]=re.sub("\n", "",splitted[i])
  answer = ''.join(splitted)
  return answer

def ok(lsnc,lword):    #function used for checking if sentence contains word
  #
  parts_o_word = len(lword)
  if parts_o_word == 1:
    return (lword[0] in lsnc) #generally checks if element of second list is in first list
  else:                  #if multiple els in second list, eg word consists of 2 words, check if they are next to each other in 1st
    for i,el in enumerate(lsnc):
      if lsnc[i:i+parts_o_word] == lword:
        return True
  return False
  #

#takes word, text, returns sentences containing the word
def important_sentences(word,sentences_text):
  head = et_nlp(word.split(' (')[0]).sentences
  head = [[w.lemma for w in s.words] for s in head][0]
  et_doc = et_nlp(sentences_text)
  lst_of_snts = et_doc.sentences
  lst_of_sntns_chosen = []
  for sentence in lst_of_snts:
    if ok([w.lemma for w in sentence.words], head):
      lst_of_sntns_chosen.append(sentence.text)                
  return lst_of_sntns_chosen

#fuction that takes sense and returns (sense, sentences containing that sense)
def A(w, s):

  if (bool(re.match("[^\s]+ \(.+\)",w)) or len(w.split()) == 1) and (bool(re.match("[^\s]+ \(.+\)",s)) or len(s.split()) == 1):
    if et_nlp(w.split()[0]).sentences[0].words[0].lemma == et_nlp(s.split()[0]).sentences[0].words[0].lemma:

      raw_text = all_pages[s]
      sentences_text = search(raw_text)
      important = important_sentences(w,sentences_text)

      if len(important) > 1:
        return (s, important)

  return 0

def fix_list(initial):
  titles_fixed = []
  for sense in initial:
    for title in all_titles:
      if sense.lower() == title.lower():
        titles_fixed.append(title)
        break
  redirect_fixed = []
  for sense in titles_fixed:
    text_of_page = all_pages[sense]
    if '#suuna' in text_of_page or '#REDIRECT' in text_of_page or '#redirect' in text_of_page:
      actual_page = re.findall('#[sunaredictREDICT]{5,8}:?\s*\[\[(.*?)\]\]\s*',text_of_page)[0].lower().strip().replace('_', ' ')
      for title in all_titles:
        if title.lower() == actual_page.lower():
          redirect_fixed.append(title)
          break
    else:
      redirect_fixed.append(sense)
  return redirect_fixed

#function that takes a WORD and list of its senses and returns (WORD, output of function A for each sense)
def B(word, senses):
  senses = fix_list(senses)
  lemma = et_nlp(word).sentences[0].words[0].lemma
  l = []
  for el in senses:
    s = et_nlp(el.split('(')[0]).sentences
    if s != []:
      if s[0].words[0].lemma == lemma:
        l.append(el)
  senses = l
  if len(senses) < 2:
    return 0
  value1 = word
  value2 = []
  for sense in senses:
    sense_and_sntnces = A(word, sense)
    if sense_and_sntnces != 0:
      value2.append(sense_and_sntnces)
  if len(value2) < 2:
    return 0
  return (value1,value2)

all_pages['Y'] = re.sub("'''Y''' on \[\[Saksamaa\]\]l '''''\[\[Bundeswehr\]\]'' < nowiki > 'i < /nowiki > ''' \[\[auto\]\]de tähis.","",all_pages['Y'])

#filtering out relevant data
nr_of_words = 0
nr_of_senses = 0
nr_of_sntnces = 0

def senses_per_word_avg(nr_of_senses,nr_of_words):
  return nr_of_senses / nr_of_words

def sntnces_per_word_avg(nr_of_sntnces,nr_of_words):
  return nr_of_sntnces / nr_of_words

def sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses):
  return nr_of_sntnces / nr_of_senses

with open("/content/drive/My Drive/Colab Notebooks/wiki_et.txt","w") as file1:
  for el in disambiguation_pages_senses:
    line = B(el[0],el[1])
    if line != 0:
     
      nr_of_words+=1
      file1.write(line[0]+":"+"\n")
      for elem in line[1]:
     
        nr_of_senses+=1
        nr_of_sntnces+=len(elem[1])

        file1.write(elem[0]+":"+" ".join(elem[1])+"\n")

print(nr_of_words)
print(nr_of_senses)
print(nr_of_sntnces)
print(round(senses_per_word_avg(nr_of_senses,nr_of_words),2))
print(round(sntnces_per_word_avg(nr_of_sntnces,nr_of_words),2))
print(round(sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses),2))