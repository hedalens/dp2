# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VgBQvy4tylE5Jw6qwK6HkMozlZEQ2wbx
"""

import bz2
import subprocess
import re
import xml.sax

pip install wikitextparser

import wikitextparser as wtp
from wikitextparser import remove_markup

pip install mwparserfromhell

import mwparserfromhell

from google.colab import drive
drive.mount('/content/drive')

class WikiXmlHandler(xml.sax.handler.ContentHandler):
    """Content handler for Wiki XML data using SAX"""
    def __init__(self):
        xml.sax.handler.ContentHandler.__init__(self)
        self._buffer = None
        self._values = {}
        self._current_tag = None
        self._pages = []

    def characters(self, content):
        """Characters between opening and closing tags"""
        if self._current_tag:
            self._buffer.append(content)

    def startElement(self, name, attrs):
        """Opening tag of element"""
        if name in ('title', 'text'):
            self._current_tag = name
            self._buffer = []

    def endElement(self, name):
        """Closing tag of element"""
        if name == self._current_tag:
            self._values[name] = ' '.join(self._buffer)

        if name == 'page':
            self._pages.append((self._values['title'], self._values['text']))
#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c

# Object for handling xml
handler = WikiXmlHandler()# Parsing object
parser = xml.sax.make_parser()
parser.setContentHandler(handler)# Iteratively process file
for line in subprocess.Popen(['bzcat'], 
                              stdin = open("/content/drive/My Drive/Colab Notebooks/etwiki-latest-pages-articles.xml.bz2"), 
                              stdout = subprocess.PIPE).stdout:
    parser.feed(line)
#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c

all_pages = dict(handler._pages)
all_titles = list(all_pages)
disambiguation_pages_texts = []
for el in handler._pages:
  is_disam_page = '{{täpsustus}}' in el[1] or '{{täpsustuslehekülg}}' in el[1] or '{{Täpsustus}}' in el[1] or '{{Täpsustuslehekülg}}' in el[1]
  if (is_disam_page) and el[0] != 'Vikipeedia:Vormistusreeglid':
    disambiguation_pages_texts.append(el)

#make list of (disambiguation page, senses)-s
disambiguation_pages_senses = []
for el in disambiguation_pages_texts:
  senses = []
  v = el[1].split('Vaata ka')[0].split('*')
  for s in v:
    links = re.findall("\[\[(.*?)\]\]",s)
    if links != []:
      senses.append(links[0].split('|')[0])
  disambiguation_pages_senses.append((el[0],senses))

pip install stanza

import stanza

stanza.download('et', verbose=False)

et_nlp = stanza.Pipeline('et', processors='tokenize,lemma', verbose=False)

#removing spaces so that parser can recognise elements
def trim_unnessessary_spaces(text):

  splitted = text.split("\n")
  for i, item in enumerate(splitted):
    splitted[i] = item.lstrip()
    splitted[i] = re.sub('< ','<',splitted[i])
    splitted[i] = re.sub(' >','>',splitted[i])
    splitted[i] = re.sub('& nbsp','&nbsp',splitted[i])
    splitted[i] = re.sub('& ndash','&ndash',splitted[i])     
  text = '\n'.join(splitted)
  return text
#function for cleaning text from markup and other, should return pure sentences in most cases
def search(text):

  regex = re.compile(r'==\s*Vaata ka\s*')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Kirjandus\s*==')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Viited\s*==')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Välislingid\s*==')
  text = regex.split(text)[0]
  text = re.sub('\[\[.*:.*(\|pisi|thumb|px).*\]\]','',text)

  code = mwparserfromhell.parse(trim_unnessessary_spaces(text))

  for tag in code.filter_tags(recursive=False):
    if tag[0] == "'" and tag[-1]=="'":
      code.replace(tag,tag.contents)
    else:
      code.replace(tag,"")

  for argument in code.filter_arguments():
    code.replace(argument, "")

  for comment in code.filter_comments():
    code.replace(comment,"")

  for external_link in code.filter_external_links():
    code.replace(external_link,"")

  for heading in code.filter_headings():
    code.replace(heading,"")

  for html_entity in code.filter_html_entities():
    code.replace(html_entity, "")

  for template in code.filter_templates(recursive=False):
    code.replace(template,"")

  return remove_markup(str(code))

def ok(lsnc,lword):    #function used for checking if sentence contains word
  
  parts_o_word = len(lword)
  if parts_o_word == 1:
    return (lword[0] in lsnc) #generally checks if element of second list is in first list
  else:                  #if multiple els in second list, eg word consists of 2 words, check if they are next to each other in 1st
    for i,el in enumerate(lsnc):
      if lsnc[i:i+parts_o_word] == lword:
        return True
  return False

#takes word, text, returns sentences containing the word
def important_sentences(word,sentences_text):
  head = et_nlp(word.split(' (')[0]).sentences
  head = [[w.lemma for w in s.words] for s in head][0]
  et_doc = et_nlp(sentences_text)
  lst_of_snts = et_doc.sentences
  lst_of_sntns_chosen = []
  for sentence in lst_of_snts:
    if ok([w.lemma for w in sentence.words], head):
      lst_of_sntns_chosen.append(sentence.text)                
  return lst_of_sntns_chosen

#fuction that takes sense and returns (sense, sentences containing that sense)
def A(word, sense):
  raw_text = all_pages[sense]
  #cleaning the text from unnecessary elements
  sentences_text = search(raw_text)
  #excluding sentences that don't contain the word
  important = important_sentences(word,sentences_text)
  #if there is at least 1 sentence containing the WORD, return (sense, these sentences, number of these sentences)
  if len(important) > 0:
    length = len(important)
    return (sense, important, length)
  #if no article about that sense return 0
  return 0

def fix_list(initial):
  titles_fixed = []
  for sense in initial:
    for title in all_titles:
      if sense.lower() == title.lower():
        titles_fixed.append(title)
        break
  redirect_fixed = []
  for sense in titles_fixed:
    text_of_page = all_pages[sense]
    if '#suuna' in text_of_page or '#REDIRECT' in text_of_page or '#redirect' in text_of_page:
      actual_page = re.findall('#[sunaredictREDICT]{5,8}:?\s*\[\[(.*?)\]\]\s*',text_of_page)[0].lower().strip().replace('_', ' ')
      for title in all_titles:
        if title.lower() == actual_page.lower():
          redirect_fixed.append(title)
          break
    else:
      redirect_fixed.append(sense)
  return redirect_fixed

#function that takes a WORD and list of its senses and returns (WORD, output of function A for each sense)
def B(word, senses):
  senses = fix_list(senses)
  value1 = word
  value2 = []  #will contain results of function A on each sense eg (sense, clean text, number of sentences)
  #filling up value2
  for sense in senses:
    sense_and_sntnces = A(word, sense)
    if sense_and_sntnces != 0: #only if there was at least 2 senses with sentences about them
      value2.append(sense_and_sntnces)
  #accept only tuples where word has at least 2 senses which have sentences containing the word
  if len(value2) < 2:
    return 0
  return (value1,value2)

all_pages['Y'] = re.sub("'''Y''' on \[\[Saksamaa\]\]l '''''\[\[Bundeswehr\]\]'' < nowiki > 'i < /nowiki > ''' \[\[auto\]\]de tähis.","",all_pages['Y'])

#filtering out relevant data
nr_of_words = 0
nr_of_senses = 0
nr_of_sntnces = 0

def senses_per_word_avg(nr_of_senses,nr_of_words):
  return nr_of_senses / nr_of_words

def sntnces_per_word_avg(nr_of_sntnces,nr_of_words):
  return nr_of_sntnces / nr_of_words

def sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses):
  return nr_of_sntnces / nr_of_senses

with open("/content/drive/My Drive/Colab Notebooks/wiki_et.txt","w") as file1:
  for el in disambiguation_pages_senses:
    line = B(el[0],el[1])
    if line != 0:
      counter = 0
      for element in line[1]:
        counter += element[2]
      if counter != len(line[1]):
        nr_of_words += 1
        nr_of_senses += len(line[1])
        file1.write('WORD: '+line[0]+'\n')
        for i,elem in enumerate(line[1]):
          nr_of_sntnces += elem[2]
          file1.write('SENSE'+str(i)+': '+elem[0]+' - ')
          file1.write(elem[1][0].strip('\n')+'\n')
          file1.write('snc: ' + str(len(elem[1])) + '\n')
        file1.write('------------------------\n')

print(nr_of_words)
print(nr_of_senses)
print(nr_of_sntnces)
print(round(senses_per_word_avg(nr_of_senses,nr_of_words),2))
print(round(sntnces_per_word_avg(nr_of_sntnces,nr_of_words),2))
print(round(sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses),2))