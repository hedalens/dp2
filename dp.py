# -*- coding: utf-8 -*-
"""Untitled8_estnltk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QAozLW9rwlD03RaMdMctaVwGGUXIXvTM
"""

import bz2
import subprocess
import re
import xml.sax

pip install wikitextparser

import wikitextparser as wtp
from wikitextparser import remove_markup

pip install mwparserfromhell

import mwparserfromhell as mw

from google.colab import drive
drive.mount('/content/drive')

class WikiXmlHandler(xml.sax.handler.ContentHandler):
    """Content handler for Wiki XML data using SAX"""
    def __init__(self):
        xml.sax.handler.ContentHandler.__init__(self)
        self._buffer = None
        self._values = {}
        self._current_tag = None
        self._pages = []

    def characters(self, content):
        """Characters between opening and closing tags"""
        if self._current_tag:
            self._buffer.append(content)

    def startElement(self, name, attrs):
        """Opening tag of element"""
        if name in ('title', 'text'):
            self._current_tag = name
            self._buffer = []

    def endElement(self, name):
        """Closing tag of element"""
        if name == self._current_tag:
            self._values[name] = ' '.join(self._buffer)

        if name == 'page':
            self._pages.append((self._values['title'], self._values['text']))
#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c

# Object for handling xml
handler = WikiXmlHandler()# Parsing object
parser = xml.sax.make_parser()
parser.setContentHandler(handler)# Iteratively process file
for line in subprocess.Popen(['bzcat'], 
                              stdin = open("/content/drive/My Drive/Colab Notebooks/etwiki-latest-pages-articles.xml.bz2"), 
                              stdout = subprocess.PIPE).stdout:
    parser.feed(line)
#src: https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c

all_pages = dict(handler._pages)
all_titles = list(all_pages)
d=[(a,b) for a,b in handler._pages if '{{täpsustus}}' in b or '{{täpsustuslehekülg}}' in b or '{{Täpsustus}}' in b or '{{Täpsustuslehekülg}}' in b]

# EstNLTK 1.4 kasutamiseks Colabis tuleb...
# 1. Laadida alla ja installeerida Miniconda
!wget https://repo.continuum.io/miniconda/Miniconda3-3.5.5-Linux-x86_64.sh
!chmod +x Miniconda3-3.5.5-Linux-x86_64.sh
!bash ./Miniconda3-3.5.5-Linux-x86_64.sh -b -f -p /usr/local
#!conda info
# 2. Installeerida pythoni sobiv versioon
!conda install -q --yes python=3.5
#!conda info
# 3. Lisada tee
import sys
sys.path.append('/usr/local/lib/python3.5/site-packages/')
# 4. Installeerida NLTK
!conda install -q -y nltk
# 5. Installeerida EstNLTK ja parandada mõned installeerimisel tekkinud vead
!conda install -q --yes -c estnltk -c conda-forge estnltk=1.4.1
!cp /usr/local/lib/python3.5/site-packages/estnltk/vabamorf/_vabamorf.cpython-35m-x86_64-linux-gnu.so /usr/local/lib/python3.5/site-packages/estnltk/vabamorf/_vabamorf.so
!cp /usr/local/lib/python3.5/site-packages/pycrfsuite/_pycrfsuite.cpython-35m-x86_64-linux-gnu.so /usr/local/lib/python3.5/site-packages/pycrfsuite/_pycrfsuite.so

import estnltk
from estnltk import Text

def trim_unnessessary_spaces(text):
  splitted = text.split("\n")
  for i, item in enumerate(splitted):
    splitted[i] = item.lstrip()
    splitted[i] = re.sub('< ','<',splitted[i])
    splitted[i] = re.sub(' >','>',splitted[i])
    s_l = re.findall('& [a-z1-9]+;',splitted[i])
    for el in s_l:
      one = el.split()[0]
      two = el.split()[1]
      three = one + two
      splitted[i] = re.sub(el,three,splitted[i])
    splitted[i] = re.sub('& #','&#',splitted[i])     
  text = '\n'.join(splitted)
  return text
def search(text):
  regex = re.compile(r'==\s*Vaata ka\s*')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Kirjandus\s*==')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Viited\s*==')
  text = regex.split(text)[0]
  regex = re.compile(r'==\s*Välislingid\s*==')
  text = regex.split(text)[0]
  code = mw.parse(trim_unnessessary_spaces(text))
 
  for tag in code.filter_tags(recursive=False):
    if tag[0] == "{" and tag[-1] == "}" or tag.tag == 'gallery' or tag.tag == 'imagemap' or tag.tag == 'center' or tag.tag == 'ref': #or tag.tag == 'table':
      code.replace(tag,"")
    else:
      code.replace(tag,tag.contents)
  
  for argument in code.filter_arguments():
    code.replace(argument, "")
  
  for comment in code.filter_comments():
    code.replace(comment,"")
  
  for external_link in code.filter_external_links():
    code.replace(external_link,"")
  
  for heading in code.filter_headings():
    code.replace(heading,"")
  
  for html_entity in code.filter_html_entities():
    code.replace(html_entity, html_entity.normalize())
  
  for template in code.filter_templates(recursive=False):
    code.replace(template,"")
  
  for wikilink in code.filter_wikilinks(recursive=False):
    if bool(re.match("(File|Fail|Pilt|Image):.+\.(SVG|svg|JPEG|jpeg|GIF|gif|PNG|png|JPG|jpg)",str(wikilink.title))):
      code.replace(wikilink,"")

  answer = remove_markup(str(code))

  splitted = answer.split('\n')
  for i,item in enumerate(splitted):
    splitted[i]=re.sub("\n", "",splitted[i])
  answer = ''.join(splitted)
  return answer

disambiguation_pages_senses = []
for page, text in d:
  senses = []
  if (bool(re.match("[^\s]+ \(.+\)",page)) or len(page.split()) == 1):
    for el in [e.title for e in mw.parse(trim_unnessessary_spaces(text)).filter_wikilinks()]:
      if (bool(re.match("[^\s]+ \(.+\)",str(el))) or len(str(el).split()) == 1):
        if Text(page.split()[0]).lemmas == Text(el.split()[0]).lemmas:
          senses.append(el)
  if len(senses) > 1:
    disambiguation_pages_senses.append((page, senses))

def ok(lsnc,lword):    #function used for checking if sentence contains word
  if lword[0] in lsnc:
    return True

def important_sentences(word,sentences_text):
  lemma = Text(word.split()[0]).lemmas
  lst_of_snts = Text(sentences_text).split_by('sentences')
  lst_of_sntns_chosen = []
  for sentence in lst_of_snts:
    if ok(sentence.lemmas,lemma):
      lst_of_sntns_chosen.append(sentence.text) 
  return lst_of_sntns_chosen

#fuction that takes sense and returns (sense, sentences containing that sense)
def A(w, s):
  raw_text = all_pages[s]
  sentences_text = search(raw_text)
  important = important_sentences(w,sentences_text)

  if len(important) > 1:
    return (s, important)

  return 0

def fix_list(initial):
  titles_fixed = []
  for sense in initial:
    for title in all_titles:
      if sense.lower() == title.lower():
        titles_fixed.append(title)
        break
  redirect_fixed = []
  for sense in titles_fixed:
    text_of_page = all_pages[sense]
    if '#suuna' in text_of_page or '#REDIRECT' in text_of_page or '#redirect' in text_of_page:
      actual_page = re.findall('#[sunaredictREDICT]{5,8}:?\s*\[\[(.*?)\]\]\s*',text_of_page)[0].lower().strip().replace('_', ' ')
      for title in all_titles:
        if title.lower() == actual_page.lower():
          redirect_fixed.append(title)
          break
    else:
      redirect_fixed.append(sense)
  return list(set(redirect_fixed))

def B(word, senses):
  senses = fix_list(senses)

  if len(senses) < 2:
    return 0

  value1 = word
  value2 = []

  for sense in senses:
    sense_and_sntnces = A(word, sense)
    if sense_and_sntnces != 0:
      value2.append(sense_and_sntnces)
  
  if len(value2) < 2:
    return 0

  return (value1,value2)

all_pages['Y'] = re.sub("'''Y''' on \[\[Saksamaa\]\]l '''''\[\[Bundeswehr\]\]'' < nowiki > 'i < /nowiki > ''' \[\[auto\]\]de tähis.","",all_pages['Y'])

#filtering out relevant data
nr_of_words = 0
nr_of_senses = 0
nr_of_sntnces = 0

def senses_per_word_avg(nr_of_senses,nr_of_words):
  return nr_of_senses / nr_of_words

def sntnces_per_word_avg(nr_of_sntnces,nr_of_words):
  return nr_of_sntnces / nr_of_words

def sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses):
  return nr_of_sntnces / nr_of_senses

with open("/content/drive/My Drive/Colab Notebooks/wiki_et.txt","w") as file1:
  for el in disambiguation_pages_senses:
    line = B(el[0],el[1])
    if line != 0:
     
      nr_of_words+=1
      file1.write(line[0]+":"+"\n")
      for elem in line[1]:
     
        nr_of_senses+=1
        nr_of_sntnces+=len(elem[1])

        file1.write(elem[0]+":"+" ".join(elem[1])+"\n")

print(nr_of_words)
print(nr_of_senses)
print(nr_of_sntnces)
print(round(senses_per_word_avg(nr_of_senses,nr_of_words),2))
print(round(sntnces_per_word_avg(nr_of_sntnces,nr_of_words),2))
print(round(sntnces_per_sense_avg(nr_of_sntnces,nr_of_senses),2))